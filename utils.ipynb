{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from pyspark import SparkContext, SQLContext, SparkConf\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "SPARK_CONTEXT: Optional[SparkContext] = None\n",
    "SQL_CONTEXT: Optional[SQLContext] = None\n",
    "\n",
    "DATA_PATH = \"/mnt\"\n",
    "\n",
    "\n",
    "def config_pyspark_submit_args():\n",
    "    sc_conf = SparkConf()\n",
    "\n",
    "    # Amount of memory to use for the driver process, i.e. where SparkContext is initialized.\n",
    "    sc_conf.set(\"spark.driver.memory\", \"100g\")\n",
    "\n",
    "    # Limit of total size of serialized results of all partitions for each Spark action (e.g. collect).\n",
    "    # Should be at least 1M, or 0 for unlimited. Jobs will be aborted if the total size is above this limit.\n",
    "    sc_conf.set(\"spark.driver.maxResultSize\", \"0\")\n",
    "\n",
    "    # Fraction of Java heap to use for aggregation and cogroups during shuffles,\n",
    "    # if spark.shuffle.spill is true. At any given time, the collective size of all in-memory maps used for shuffles\n",
    "    # is bounded by this limit, beyond which the contents will begin to spill to disk.\n",
    "    # If spills are often, consider increasing this value at the expense of spark.storage.memoryFraction.\n",
    "    sc_conf.set(\"spark.shuffle.memoryFraction\", \"0.3\")\n",
    "\n",
    "    # Amount of memory to use per python worker process during aggregation,\n",
    "    # in the same format as JVM memory strings (e.g. 512m, 2g).\n",
    "    # If the memory used during aggregation goes above this amount, it will spill the data into disks\n",
    "    sc_conf.set(\"spark.python.worker.memory\", \"8g\")\n",
    "\n",
    "    # By default, Spark serializes objects using Java’s ObjectOutputStream framework,\n",
    "    # and can work with any class you create that implements java.io.Serializable.\n",
    "    # You can also control the performance of your serialization more closely by extending java.io.Externalizable.\n",
    "    # Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.\n",
    "    # Kryo serialization: Spark can also use the Kryo library (version 4) to serialize objects more quickly.\n",
    "    # Kryo is significantly faster and more compact than Java serialization (often as much as 10x),\n",
    "    # but does not support all Serializable types and requires you to register the classes you’ll use in the program\n",
    "    # in advance for best performance.\n",
    "    sc_conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    sc_conf.set(\"spark.kryoserializer.buffer.max\", \"2000\")\n",
    "\n",
    "    # expresses the size of M as a fraction of the (JVM heap space - 300MB) (default 0.6).\n",
    "    # The rest of the space (40%) is reserved for user data structures, internal metadata in Spark,\n",
    "    # and safeguarding against OOM errors in the case of sparse and unusually large records.\n",
    "    # For more info: https://spark.apache.org/docs/latest/tuning.html\n",
    "    # another definition:\n",
    "    # Fraction of (heap space - 300MB) used for execution and storage.\n",
    "    # The lower this is, the more frequently spills and cached data eviction occur.\n",
    "    # The purpose of this config is to set aside memory for internal metadata, user data structures,\n",
    "    # and imprecise size estimation in the case of sparse, unusually large records.\n",
    "    # Leaving this at the default value is recommended.\n",
    "    sc_conf.set(\"spark.memory.fraction\", \"0.4\")\n",
    "\n",
    "    # Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by\n",
    "    # spark.memory.fraction. The higher this is, the less working memory may be available to execution\n",
    "    # and tasks may spill to disk more often. Leaving this at the default value is recommended.\n",
    "    sc_conf.set(\"spark.memory.storageFraction\", \"0.5\")\n",
    "\n",
    "    # some configs for default dirs\n",
    "    sc_conf.set(\"spark.local.dir\", f\"{DATA_PATH}/spark_tmp\")\n",
    "    sc_conf.set(\"spark.executor.extraJavaOptions\", f\"-Djava.io.tmpdir={DATA_PATH}/spark_tmp\")\n",
    "    sc_conf.set(\"spark.driver.extraJavaOptions\", f\"-Djava.io.tmpdir={DATA_PATH}/spark_tmp\")\n",
    "\n",
    "    # Apache Arrow is an in-memory columnar data format that is used in Spark to efficiently transfer\n",
    "    # data between JVM and Python processes. This currently is most beneficial to Python users\n",
    "    # that work with Pandas/NumPy data.\n",
    "    sc_conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n",
    "    # Configures the number of partitions that are used when shuffling data for joins or aggregations.\n",
    "    # Recommended - 2-3x of virtual machine cores, but < 128Mb of data for each partition\n",
    "    sc_conf.set(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "\n",
    "    return sc_conf\n",
    "\n",
    "\n",
    "def setup_context():\n",
    "    global SPARK_CONTEXT\n",
    "    global SQL_CONTEXT\n",
    "\n",
    "    config = config_pyspark_submit_args()\n",
    "\n",
    "    SPARK_CONTEXT = SparkContext(conf=config)\n",
    "    SQL_CONTEXT = SQLContext(SPARK_CONTEXT)\n",
    "\n",
    "    logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "\n",
    "    SPARK_CONTEXT.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # Checkpointing is actually a feature of Spark Core(that SparkSQL uses for distributed computations)\n",
    "    # that allows a driver to be restarted on failure\n",
    "    # with previously computed state of a distributed computation described as an RDD\n",
    "    # Checkpointing truncates the lineage of a RDD to be checkpointed.\n",
    "    # That has been successfully used in Spark MLlib in iterative machine learning algorithms like ALS\n",
    "    # https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-checkpointing.html\n",
    "\n",
    "    checkpoint_path = f\"{DATA_PATH}/checkpoint/\"\n",
    "\n",
    "    SPARK_CONTEXT.setCheckpointDir(checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "def get_sql_context() -> SQLContext:\n",
    "    if not SQL_CONTEXT:\n",
    "        setup_context()\n",
    "    return SQL_CONTEXT\n",
    "\n",
    "    \n",
    "def stop_context():\n",
    "    SPARK_CONTEXT.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from io import BytesIO\n",
    "from typing import List, Union, Optional\n",
    "import logging\n",
    "import pyspark.sql.functions as SF\n",
    "from pyspark.sql import DataFrame, DataFrameWriter\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "\n",
    "def cast_column_type(df1: DataFrame, df2: DataFrame, column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    cast given column of df2 to the type of the column in df1\n",
    "    :param df1: main dataframe with \"good\" data type\n",
    "    :param df2: secondary dataframe which iw going to change his column type\n",
    "    :param column: column to change\n",
    "    :return: df2 with new column data type\n",
    "    \"\"\"\n",
    "\n",
    "    # here we get needed column from df1 with type\n",
    "    needed_column_attrs = list(filter(lambda x: x.name == column, list(df1.schema)))\n",
    "\n",
    "    if len(needed_column_attrs) == 0:\n",
    "        raise ValueError(f\"Column  {column} for cast doesn't exists\")\n",
    "\n",
    "    needed_column_type = needed_column_attrs[0].dataType\n",
    "\n",
    "    return df2.withColumn(column, df1[column].cast(needed_column_type))\n",
    "\n",
    "\n",
    "def unify_dataframe_columns(main_df: DataFrame, second_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    unifies all data types of all columns of second_df to the all data types of main_df\n",
    "    :param main_df: main dataframe with \"good\" data types of columns\n",
    "    :param second_df: secondary dataframe which iw going to change his columns types\n",
    "    :return: second_df with new unified columns to the main_df\n",
    "    \"\"\"\n",
    "\n",
    "    main_df_columns = list(main_df.schema)\n",
    "    second_df_columns = list(main_df.schema)\n",
    "\n",
    "    if set([c.name for c in main_df_columns]) != set([c.name for c in second_df_columns]):\n",
    "        raise ValueError(f\"Columns from given dataframes are not the same\")\n",
    "\n",
    "    # here we sort columns for the right order in next cycle\n",
    "    main_df_columns = list(sorted(main_df_columns, key=lambda x: x.name))\n",
    "    second_df_columns = list(sorted(second_df_columns, key=lambda x: x.name))\n",
    "\n",
    "    for i in range(len(main_df_columns)):\n",
    "        # if data types of columns don't match - do cast second dataframe column\n",
    "        if main_df_columns[i].dataType != second_df_columns[i].dataType:\n",
    "            logging.warning(f\"data types of column {main_df_columns[i].name} doesn't match:\"\n",
    "                           f\" {main_df_columns[i].dataType}, {second_df_columns[i].dataType}\")\n",
    "            second_df = cast_column_type(main_df, second_df, main_df_columns[i].name)\n",
    "\n",
    "    return second_df\n",
    "\n",
    "\n",
    "def concatenate(\n",
    "        dfs: List[DataFrame],\n",
    "        cast_types_to_first_element=False,\n",
    "        columns: Optional[List[str]] = None\n",
    ") -> DataFrame:\n",
    "    # sanity check for Nones\n",
    "    dfs = list(filter(lambda df: df is not None, dfs))\n",
    "\n",
    "    if columns is None:\n",
    "        columns = list(set(sum(map(lambda x: list(x.columns), dfs), [])))\n",
    "\n",
    "    for i in range(len(dfs)):\n",
    "        for column in columns:\n",
    "            if column not in dfs[i].columns:\n",
    "                dfs[i] = dfs[i].withColumn(column, SF.lit(None).cast(StringType()))\n",
    "\n",
    "    if len(dfs) > 1 and cast_types_to_first_element:\n",
    "        for i in range(1, len(dfs)):\n",
    "            dfs[i] = unify_dataframe_columns(dfs[0], dfs[i])\n",
    "\n",
    "    return reduce(DataFrame.unionByName, dfs)\n",
    "\n",
    "def read_csv(path: str) -> DataFrame:\n",
    "    return get_sql_context() \\\n",
    "        .read.format(\"com.databricks.spark.csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(path)\n",
    "\n",
    "\n",
    "def write_csv(df: DataFrame, path: str, single=False) -> None:\n",
    "    if single:\n",
    "        df = df.coalesce(1)\n",
    "\n",
    "    df \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .format(\"com.databricks.spark.csv\") \\\n",
    "        .save(path, index=\"False\")\n",
    "\n",
    "\n",
    "def read_table(path: str, table_name: str) -> DataFrame:\n",
    "    return get_sql_context() \\\n",
    "        .read \\\n",
    "        .option(\"path\", path) \\\n",
    "        .table(table_name)\n",
    "\n",
    "\n",
    "def write_table(\n",
    "        df: DataFrame, path: str, table_name: str,\n",
    "        buckets: int = 20, bucket_columns: Union[str, List[str]] = None, mode: str = \"overwrite\",\n",
    "        sort_column: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    :param df: spark Dataframe object\n",
    "    :param path:\n",
    "    :param table_name:\n",
    "    :param buckets: number of buckets\n",
    "    :param bucket_columns: columns for bucketing\n",
    "    :param mode: spark write mode\n",
    "    :param sort_column: column for sorting\n",
    "\n",
    "    bucketing use for saving partitioned parquet files and join optimization\n",
    "    https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-bucketing.html\n",
    "    \"\"\"\n",
    "    writer_obj: DataFrameWriter = df.write\n",
    "\n",
    "    writer_obj = writer_obj.mode(mode)\n",
    "\n",
    "    if bucket_columns is not None:\n",
    "        writer_obj = writer_obj.bucketBy(buckets, bucket_columns)\n",
    "\n",
    "    if sort_column is not None:\n",
    "        writer_obj = writer_obj.sortBy(sort_column)\n",
    "\n",
    "    writer_obj \\\n",
    "        .option(\"path\", validate_cluster_path(path)) \\\n",
    "        .saveAsTable(table_name)\n",
    "\n",
    "\n",
    "def read_parquet(path: str) -> DataFrame:\n",
    "    return get_sql_context().read.parquet(path)\n",
    "\n",
    "\n",
    "def write_parquet(df: DataFrame, path: str, mode=\"overwrite\", single=False):\n",
    "    if single:\n",
    "        df = df.coalesce(1)\n",
    "    \n",
    "    df \\\n",
    "        .write \\\n",
    "        .mode(mode) \\\n",
    "        .parquet(path)\n",
    "\n",
    "def check_path_exists(path: str, is_spark_dir: bool = False):\n",
    "    path_exists = os.path.exists(path)\n",
    "\n",
    "    if is_spark_dir:\n",
    "        path_exists = path_exists and \\\n",
    "            check_path_exists(os.path.join(path, \"_SUCCESS\"), is_spark_dir=False)\n",
    "\n",
    "    return path_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def save_pickle(path, obj, *args, **kwargs):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f, *args, **kwargs)\n",
    "\n",
    "def load_pickle(path, required: bool = False):\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "def save_json(path, obj, use_np_encoder: bool = True):\n",
    "    encoder_class = None\n",
    "    if use_np_encoder:\n",
    "        encoder_class = NpEncoder\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, cls=encoder_class)\n",
    "\n",
    "\n",
    "def load_json(path, required: bool = False):\n",
    "    with open(path, \"r\") as f:\n",
    "        obj = json.load(f)\n",
    "\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def read_pandas_from_spark(path, read_function = pd.read_csv, file_type=\"csv\"):\n",
    "    if os.path.isfile(path):\n",
    "        return read_function(path)\n",
    "    else:\n",
    "        files = list(os.walk(path))[0][2]\n",
    "\n",
    "        dataframes = []\n",
    "\n",
    "        for file in tqdm_notebook(files):\n",
    "            if not file.startswith(\".\") and file.endswith(f\".{file_type}\"):\n",
    "                try:\n",
    "                    file_path = os.path.join(path, file)\n",
    "                    df = read_function(file_path)\n",
    "                    dataframes.append(df)\n",
    "\n",
    "                except:\n",
    "                    print(f\"can't read file - {file} from spark csv\")\n",
    "                    continue\n",
    "        \n",
    "        if len(dataframes) == 1:\n",
    "            return dataframes[0]\n",
    "        \n",
    "        result = pd.concat(dataframes, copy=False).reset_index(drop=True)\n",
    "\n",
    "        return result\n",
    "    \n",
    "def read_pandas_parquet_from_spark(path):\n",
    "    return read_pandas_from_spark(path, read_function=pd.read_parquet, file_type=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
